{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9a52a3f-6f4e-43fe-b110-100845123fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.functions import count, col, expr, lit, when, concat, sum, avg, max, min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dcad168-14e6-4009-9d01-eebda650fa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Define schema for our data using DDL\n",
    "schema = \"`Id` INT, `First` STRING, `Last` STRING, `Url` STRING, `Published` STRING, `Hits` INT, `Campaigns` ARRAY<STRING>, `Notes` STRING\"\n",
    "# Create our static data\n",
    "data = [\n",
    "    [1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\", \"LinkedIn\"], \"column that is not in schema\"],\n",
    "    [2, \"Brooke\",\"Wenig\", \"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\", \"LinkedIn\"], \"column that is not in schema\"],\n",
    "    [3, \"Denny\", \"Lee\", \"https://tinyurl.3\", \"6/7/2019\", 7659, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"], \"column that is not in schema\"],\n",
    "    [4, \"Tathagata\", \"Das\", \"https://tinyurl.4\", \"5/12/2018\", 10568,[\"twitter\", \"FB\"], \"column that is not in schema\"],\n",
    "    [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"], \"column that is not in schema\"],\n",
    "    [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568, [\"twitter\", \"LinkedIn\"], \"column that is not in schema\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "431168e1-9f97-4eb3-8967-cebb8f94dad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/19 11:41:35 WARN Utils: Your hostname, antonov-mac-dev.local resolves to a loopback address: 127.0.0.1; using 192.168.18.243 instead (on interface en0)\n",
      "24/12/19 11:41:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/19 11:41:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|               Notes|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+--------------------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|column that is no...|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|column that is no...|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|column that is no...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|column that is no...|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|column that is no...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|column that is no...|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+--------------------+\n",
      "\n",
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- First: string (nullable = true)\n",
      " |-- Last: string (nullable = true)\n",
      " |-- Url: string (nullable = true)\n",
      " |-- Published: string (nullable = true)\n",
      " |-- Hits: integer (nullable = true)\n",
      " |-- Campaigns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- Notes: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 61564)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/Users/tonkata/Library/Python/3.9/lib/python/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/Users/tonkata/Library/Python/3.9/lib/python/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/Users/tonkata/Library/Python/3.9/lib/python/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/Users/tonkata/Library/Python/3.9/lib/python/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create a SparkSession \n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Example-3_6\")\n",
    "         .getOrCreate())\n",
    "# Create a DataFrame using the schema defined above\n",
    "blogs_df = spark.createDataFrame(data, schema)\n",
    "# Show the DataFrame; it should reflect our table above \n",
    "blogs_df.show()\n",
    "# Print the schema used by Spark to process the DataFrame \n",
    "print(blogs_df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7be55868-2691-407d-8fcf-00d2edc833c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('Id', IntegerType(), True), StructField('First', StringType(), True), StructField('Last', StringType(), True), StructField('Url', StringType(), True), StructField('Published', StringType(), True), StructField('Hits', IntegerType(), True), StructField('Campaigns', ArrayType(StringType(), True), True), StructField('Notes', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "print(blogs_df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8d2234d-78e8-43b3-9c2c-6503cfe120ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Id', 'First', 'Last', 'Url', 'Published', 'Hits', 'Campaigns', 'Notes']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogs_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca85be22-2f29-44dd-b584-70913e0f4849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'Id'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col('Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "694a0b7c-f7ca-4f12-beb0-cfad172bcb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Do aritmetics on columns\n",
    "blogs_df.select(col(\"Hits\") * 2).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6360eb26-15e5-4a4c-9eae-4e8aa6251e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Do aritmetics on columns\n",
    "blogs_df.select(expr(\"Hits * 2\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "715cb76a-ce5c-4e77-8c10-4fcdac63933b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+--------------------+-----------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|               Notes|Big Hitters|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+--------------------+-----------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|column that is no...|      false|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|column that is no...|      false|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|column that is no...|      false|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|column that is no...|       true|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|column that is no...|       true|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|column that is no...|       true|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adds a calculated column at the end of the DataFrame \n",
    "blogs_df.withColumn(\"Big Hitters\", (expr(\"Hits > 10000\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57dfd303-aa1a-4c5f-b0be-4f2bdc6ea6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|    AuthorsId|\n",
      "+-------------+\n",
      "|  JulesDamji1|\n",
      "| BrookeWenig2|\n",
      "|    DennyLee3|\n",
      "|TathagataDas4|\n",
      "+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# expr here is gettng the column value\n",
    "# Example concatenates few columns together and then selects that column. This should not modify the original DF\n",
    "blogs_df.withColumn(\"AuthorsId\", (concat(expr(\"First\"), expr(\"Last\"), expr(\"Id\")))).select(col(\"AuthorsId\")).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea159e05-6528-48dd-96bf-91637d2a9b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Same results for all. col and expr are the same method call\n",
    "blogs_df.select(expr(\"Hits\")).show(2)\n",
    "blogs_df.select(col(\"Hits\")).show(2)\n",
    "blogs_df.select(\"Hits\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "524d575b-b959-4f49-b3bd-475ce139d279",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Sort by column \"Id\" in descending order\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mblogs_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mId\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pyspark/sql/dataframe.py:2742\u001b[0m, in \u001b[0;36mDataFrame.sort\u001b[0;34m(self, *cols, **kwargs)\u001b[0m\n\u001b[1;32m   2651\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msort\u001b[39m(\n\u001b[1;32m   2652\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: Union[\u001b[38;5;28mstr\u001b[39m, Column, List[Union[\u001b[38;5;28mstr\u001b[39m, Column]]], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m   2653\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2654\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a new :class:`DataFrame` sorted by the specified column(s).\u001b[39;00m\n\u001b[1;32m   2655\u001b[0m \n\u001b[1;32m   2656\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2740\u001b[0m \u001b[38;5;124;03m    +---+-----+\u001b[39;00m\n\u001b[1;32m   2741\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2742\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39msort(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sort_cols\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pyspark/sql/dataframe.py:2779\u001b[0m, in \u001b[0;36mDataFrame._sort_cols\u001b[0;34m(self, cols, kwargs)\u001b[0m\n\u001b[1;32m   2777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cols[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m   2778\u001b[0m     cols \u001b[38;5;241m=\u001b[39m cols[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 2779\u001b[0m jcols \u001b[38;5;241m=\u001b[39m [_to_java_column(cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m, c)) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols]\n\u001b[1;32m   2780\u001b[0m ascending \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mascending\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ascending, (\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m)):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pyspark/sql/dataframe.py:2779\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cols[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m   2778\u001b[0m     cols \u001b[38;5;241m=\u001b[39m cols[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 2779\u001b[0m jcols \u001b[38;5;241m=\u001b[39m [\u001b[43m_to_java_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mColumnOrName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols]\n\u001b[1;32m   2780\u001b[0m ascending \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mascending\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ascending, (\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m)):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pyspark/sql/column.py:65\u001b[0m, in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     63\u001b[0m     jcol \u001b[38;5;241m=\u001b[39m _create_column_from_name(col)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m     66\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN_OR_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     67\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m     68\u001b[0m     )\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jcol\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got method."
     ]
    }
   ],
   "source": [
    "#Sort by column \"Id\" in descending order\n",
    "\n",
    "blogs_df.sort(col('Id').desc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
